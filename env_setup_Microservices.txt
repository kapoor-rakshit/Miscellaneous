Microservices Architecture
============================
  Client <---> Load Balancers <---> Application Servers <---> Cache <---> Sharded Databases
- It is an approach to design and implement applications in which large application is built from modular components or services.
- Each module supports a specific task or business goal and uses a well-defined communications interface,
  such as an application programming interface (API), to communicate with other modules and services.
- It is a departure from applications that were coded into a single executable application i.e. monolithic
  Each service can be isolated, rebuilt, tested, redeployed and managed independently.
  eg: if program isn't properly generating reports we can test, restart, patch, redeploy that service only, independent of other services.
- Typical characteristics of microservices design and architecture include:
  - Unique components (individual components for specific function)
  - Decentralized (few dependencies and requires frequent and extensive communication between components)
  - Resilient (fault tolerance)
  - API-based (APIs and API gateways for communication between components and other applications)
  - Data separation (each service accesses its own database or storage volume)
  - Automation (automation and orchestration technologies for component deployment and scaling)
- Challenges of a microservices architecture:
  - Unnecessary complexity
  - Performance issues from Network congestion, latency, network management, control can require multiple load-balancing instances
  - Monitoring health and performance, as well as automation and orchestration
  - Security

WebSocket
============
WebSockets are a technology that allow for real-time communication between a client and server.
They provide a bi-directional communication, without the need for the client to constantly request data from the server.
In traditional web applications, the client sends a request to the server, which then responds with the requested data.
This request-response model can be inefficient for real-time applications, such as
chat applications, online gaming, and financial trading platforms, where data needs to be updated in real-time.
REFER: CODE SAMPLE IN   `angular`   REPOSITORY's   `misc`

Server Sent Events or EventSource
====================================
Similar to WebSocket, the connection is persistent.
But there are several important differences:
- WebSocket
  Both client and server can exchange messages | Binary and text data | WebSocket protocol (wss:// or ws://) | Manual code for reconnect
- EventSource
  Only server sends data | Only text | Regular HTTP (https:// or http://) | Supports auto-reconnect
The server can set recommended delay using `retry:` in response (in milliseconds). There’s no way to set it from JavaScript.
The server may specify custom events using `event:` (default type is message). event name, must precede data:
CODE SAMPLE:
- SERVER
  retry: 15000
  event: join
  data: {"KEY1": "VAL1", "KEY2": "VAL2"}                              // join event
  data: {"KEY1": "VAL1", "KEY2": "VAL2", "KEY3": "VAL3"}              // message event
  event: leave
  data: {"KEY1": "VAL1"}                                              // leave event
- JS
  let eventSource = new EventSource("https://another-site.com/events");
  eventSource.onmessage = function(event) {                           // onmessage: similar to WebWorker
    console.log("New message", event.data);
  };
  eventSource.close();

Polling - Long / Short
=========================
Polling is the simplest way of having persistent connection with server,
that doesn’t use any specific protocol like WebSocket or Server Sent Events.
A.Regular Polling or Short Polling
  Regular requests are sent to server: “Hello, I’m here, do you have any information for me?”. For example, once every 10 seconds.
  In response, server takes notice to itself that client is online, and then sends a packet of messages it got till that moment.
  It has some downsides:
  - Messages are passed with a delay up to N seconds (here 10 seconds, between requests).
  - Even if there are no messages, the server is bombed with requests every N seconds,
    even if the user switched somewhere else or is asleep. That’s quite a load performance-wise.
B.Long Polling
  A request is sent to the server. The server doesn’t close the connection until it has a message to send.
  When a message appears – the server responds to the request with it.
  The browser makes a new request immediately.
  Only when a message is delivered, the connection is closed and reestablished.
  If the connection is lost, because of, say, a network error, the browser immediately sends a new request.
  CODE SAMPLE for long polling, where subscribe function makes fetch, then waits for response, handles it and calls itself again:
  async function subscribe() {
    let response = await fetch("/subscribe");
    if (response.status == 502) {
      // Status 502 is a connection timeout error,
      // may happen when the connection was pending for too long, and the remote server or a proxy closed it
      // let's reconnect
      await subscribe();
    } else if (response.status != 200) {
      // An error - let's show it
      showMessage(response.statusText);
      // Reconnect in one second
      await new Promise(resolve => setTimeout(resolve, 1000));
      await subscribe();
    } else {
      // Get and show the message
      let message = await response.text();
      showMessage(message);
      // Call subscribe() again to get the next message
      await subscribe();
    }
  }
  subscribe();

Rate Limiting
===============
It is a technique used in applications to control and manage the rate at which requests are processed by individual services.
The primary objective of implementing rate limiting in microservices is to prevent overloading, maintain service stability and
availability, and mitigate potential security threats, such as Denial of Service (DoS) attacks.
Rate limiting can be applied at different levels, such as API gateway, load balancer, or even on an individual microservice level.
There are different approaches to implementing rate limiting, some of the popular ones being:
- Fixed Window: In this approach, fixed number of requests are allowed per predefined time window (e.g., 1000 requests per minute).
  The disadvantage of this approach is that it can lead to uneven distribution of requests and, in some cases, cause service overload.
- Sliding Window: Rate limit is gradually adjusted based on number of requests received in a recent time window.
  This approach offers better control over the rate of incoming requests.
- Token Bucket: In this method, tokens are generated at a specific rate and added to a bucket.
  Each incoming request consumes a token from the bucket. If there are no tokens available, the request is denied.
  This technique allows for short bursts of requests while maintaining an overall balanced rate

Scaling - Horizontal / Vertical
=================================
- Horizontal scaling involves adding more machines or nodes to a system. It involves scaling out.
  Horizontal scaling is typically used to handle increasing amounts of traffic or workload.
- Vertical scaling involves adding more power (CPU, RAM, storage, etc.) to an existing machine. It involves scaling up.
  Vertical scaling is typically used to handle resource-intensive tasks or applications that require more processing power.

Data Partitioning / Sharding
==============================
- Partitioning is an optimization technique­ in databases where a single­ table is divided into smaller se­gments called partitions.
  It is within a single database instance. It is then used with join queries.
- Sharding involves fragmenting the­ extensive datase­t into smaller, self-contained se­gments known as shards.
  These shards are­ then allocated to separate­ servers or nodes, facilitating paralle­lism in data processing.
  As a result, query re­sponse times are improve­d, high traffic loads can be accommodated.

Service Discovery
===================
In case of a micro-service architecture that is deployed in cloud, there will be a lot of microservice components for different services.
Every microservice can scale differently based on the demands.
Eg: The Order service might have 4 or 5 instances and Billing Service might have 2 or 3 instances running.
With each instance having a dynamic network address because of multiple factors like Autoscaling, Upgrade, Failure, Deployments, etc.
it is extremely difficult to locate these services and communicate with them.
Before server connects to other server it needs to know IP address and port number where this particular application is running in server. 
This problem leads to the solution in the form of a design pattern called Service Discovery.
We need service discovery to find the hosts and IP info and
If multiple hosts are available then the load balancer helps to pick one whoever has less load and make a call in a load-balanced way.
Eg: Whenever Service-A and Service-B start off, they register themselves inside the discovery service's service registry.
    Service Registry is a database containing network locations of each service's instances.
    Service-A wants to connect to Service-B.
    Now the load balancer once get the request, it is gonna do a query with the discovery service that,
    hey, can you tell me what instances are there for Service-B?
    Now the load balancer finds out that there are this many instances available where Service-B has been deployed.
    Load Balancer is going to dispatch to one of the servers by looking into Service Registry.
    It can take all four instances of Service-B and whoever has less load then to balance the load, it can send the request to there.
There are two types of Service Discovery
- Client-Side Service Discovery like Zookeeper, Consul
  Eg: Service-A wants to connect to Service-B then Service-A is asking directly to Discovery Service and
      Discovery Service is providing the URLs or the port number of all instances of Service-B.
      There is no load balancer and client(Service-A) is doing load balancing and calling the Service-B instance by itself.
  Advantage: it saves an extra hop that we would’ve had with a dedicated load balancer.
  Disadvantage: because the Service Consumer must implement the load balancing logic.
- Server-Side Service Discovery like NGINX, AWS ELB
  The default scenario explained above. Here Service-A(Client) doesn’t talk to the Discovery Service directly.
  It calls another server (Load Balancer) which helps to discover Service-B URL info.
  Advantage: load Balancer does the job of load balancing.
  Disadvantage: set up and operate the Load Balancer.

Circuit Breaker Pattern
=========================
If there are many callers to unresponsive service, we can run out of critical resources (CPU, threads) leading to cascading failures across multiple systems.
In a microservice system, failing fast is critical. If there are failures in the Microservice ecosystem, then you need to fail fast by opening the circuit.
This ensures that no additional calls are made to the failing service so that we return an exception immediately.
This pattern also monitors the system for failures and, once things are back to normal, the circuit is closed to allow normal functionality.
It can have three states:
- When no. of consecutive failures crosses threshold, circuit breaker trips,
  and for duration of timeout period all attempts to invoke remote service will fail immediately. (Open)
- After the timeout expires the circuit breaker allows a limited number of test requests to pass through (Half-Open)
- If those requests succeed the circuit breaker resumes normal operation. (Closed)
  Otherwise, if there is a failure the timeout period begins again.

MapReduce
===========
MapReduce is a programming model used for efficient processing in parallel over large data-sets in a distributed manner.
The data is first split and then combined to produce the final result.

Message Queues
================
Message queues allow microservices to communicate asynchronously and reliably. Eg: Kafka, RabbitMQ
A message queue, is a queue that routes messages from sender to receiver, (from service A to service B) following FIFO (First In, First Out) policy.
This approach decouples sender and receiver, allowing each service to operate independently without being tightly coupled to others.
Messages can be stored in a queue when the receiving service is not immediately available to process them, helping with reduction of load on services
Two main message queue models:
- Point-to-point (P2P)
  a message published by producer is received only by one consumer.
- Publish-subscribe (Pub/Sub)
  a single or multiple producers broadcast messages to multiple consumers.
  eg: placing order on website, information needs to be processed and sent to services, like inventory management, payment processing, shipping logistics.

Load Balancers
================
- When a microservices function requires more computing power,
  only that microservice is scaled by adding more instances of that service through load balancer to share network traffic.
- We can add a Load balancing layer at three places in our system:
  Between Clients and Application servers
  Between Application Servers and database servers
  Between Application Servers and Cache servers
- Round Robin approach distributes incoming requests equally among backend servers.
  If any server is dead, LB will take it out of rotation and will stop sending any traffic to it.
- A problem with Round Robin LB is that we don’t take the server load into consideration.
  If a server is overloaded or slow, LB will not stop sending new requests to that server.
  To handle this, another LB solution can be used that periodically queries backend server about its load & adjusts traffic based on that.

Web Authentication and Security
=================================
The API gateway is a single endpoint entry for all requests.
Instead of having access to multiple services, a client sends a request to the API gateway, responsible for routing it to downstream service.
It is thus an excellent candidate to enforce authentication concerns.
- Stateful authentication
  The server creates a session for the user after successfully authenticating. The session id is then stored as a cookie in the user's browser and
  the user session store in the cache or database. When the client tries to access the server with a given session id,
  the server attempts to load the user session context for the session store, checks if the session is valid, and
  decides if the client has to access the desired resource or rejects the request.
- Stateless authentication
  It stores the user session on the client-side. A cryptographic algorithm signs the user session to ensure the session data’s integrity and authority.
  Each time client requests a resource from server, server is responsible for verifying the token claims sent as a cookie.
  Since the user session is stored on the client-side, this approach frees the overhead to maintain the user session state.
  JWT
  The JWT(JSON Web Token) token is a signed JSON object that contains a list of claims which allow the receiver to validate the sender's identity.
  The purpose of using the JWT token is for a stateless authentication mechanism.
  JWT Structure  , eg: eyJhbGciOiJI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIM5MDIyfQ.SflKxwRJSMyJV_adQssw5c
  The JSON Web token is composed of three parts separated by periods:
  - header: contains the algorithm used for signing.
  - payload: is session data that also refers to ‘claims’. There are two types of claims:
    - reserved claims that are recommended to use while generating the JWT token.
    - custom claims
  - signature: is calculated by encoding the header and the payload using Base64 encoded.
    Then the encode64 is signed using a secret key and cryptographic algorithms specified in the header section.
    The signature is used to verify the token has not changed or modified.

Indexing
==========
Indexing improves database performance by minimizing number of disc visits required to fulfill a query.
It is a data structure technique used to locate and quickly access data in databases. Several database fields are used to generate indexes.
The main key or candidate key of the table is duplicated in the first column, which is the Search key.
The second column is Data Reference or Pointer which contains set of pointers holding address of disk block where that particular key value can be found.

CAP Theorem
=============
The CAP theorem states that it is not possible to guarantee all three of the desirable properties.
As a result, database systems prioritize only two properties at a time.: consistency, availability, and partition tolerance.
- Consistency
  all clients see same data at same time, no matter path of their request. This is critical for applications that do frequent updates.
- Availability
  all functioning application components will return a valid response, even if they are down.
- Partition Tolerance
  application will operate even during a network failure that results in lost or delayed messages between services.
  This comes into play for applications that integrate with a large number of distributed, independent components.

Deployment Types - Rolling / Blue Green / Canary
==================================================
- Rolling Deployment
  It updates running instances of an application with new release.
  All nodes in a target environment are incrementally updated with the service or artifact version in integer N batches.
  Since nodes are updated in batches, rolling deployments require services to support both new and old versions of an artifact.
  Verification of an application deployment at every incremental change also makes this deployment slow.
- Blue-Green Deployment
  It involves maintaining two microservice variations simultaneously in production.
  One microservice version (the green microservice) is visible to the user and gets traffic.
  The other one (the blue microservice) remains idle for developers to make updates.
  A microservice remains in blue state until it passes tests and is ready to go out. After passing all tests, microservices move to green state.
  There’s no downtime during development and deployment because there’s always another stable variation serving production traffic.
  In addition, if new deployment isn’t working correctly, we can quickly roll back to previous variation (i.e., the blue microservice).
- Canary Deployment
  It includes releasing microservice with new version to only small percentage of load first and seeing if it works as expected.
  As microservice passes through rigorous testing, it gradually encounters larger workloads.
  If canaries aren’t functioning correctly, traffic may be routed to a stable version while the problem is investigated and debugged.
  The canary deployment strategy releases only one microservice at a time.
  Microservices with higher criticality and risks involved can be made available before others.
  It improves availability by detecting problems early, before a critical microservice is exposed to the entire system.

A/B Testing
=============
A/B Testing is used to test two application versions at same time.
By splitting traffic 50/50 between two versions of an application/website, to determine which version is more popular, faster, a better fit, etc.
A/B testing directs traffic randomly between the two versions, and logs, traces, and monitoring are used to define the results.

Edge Caching
================
Edge caching is a caching technique that stores frequently accessed data closer to the end-users on servers closer to them,
rather than being stored in a central data center.
Edge caching is used to reduce latency that can occur when data is transmitted over long distances, which slows down application performance.
