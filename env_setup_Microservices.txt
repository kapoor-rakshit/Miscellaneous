Microservices Architecture
============================
  Client <---> Load Balancers <---> Application Servers <---> Cache <---> Sharded Databases
- It is an approach to design and implement applications in which large application is built from modular components or services.
- Each module supports a specific task or business goal and uses a well-defined communications interface,
  such as an application programming interface (API), to communicate with other modules and services.
- It is a departure from applications that were coded into a single executable application i.e. monolithic
  Each service can be isolated, rebuilt, tested, redeployed and managed independently.
  eg: if program isn't properly generating reports we can test, restart, patch, redeploy that service only, independent of other services.
- Typical characteristics of microservices design and architecture include:
  - Unique components (individual components for specific function)
  - Decentralized (few dependencies and requires frequent and extensive communication between components)
  - Resilient (fault tolerance)
  - API-based (APIs and API gateways for communication between components and other applications)
  - Data separation (each service accesses its own database or storage volume)
  - Automation (automation and orchestration technologies for component deployment and scaling)
- Challenges of a microservices architecture:
  - Unnecessary complexity
  - Performance issues from Network congestion, latency, network management, control can require multiple load-balancing instances
  - Monitoring health and performance, as well as automation and orchestration
  - Security

WebSocket
============
WebSockets are a technology that allow for real-time communication between a client and server.
They provide a bi-directional communication, without the need for the client to constantly request data from the server.
In traditional web applications, the client sends a request to the server, which then responds with the requested data.
This request-response model can be inefficient for real-time applications, such as
chat applications, online gaming, and financial trading platforms, where data needs to be updated in real-time.
REFER: CODE SAMPLE IN   `angular`   REPOSITORY's   `misc`

Server Sent Events or EventSource
====================================
Similar to WebSocket, the connection is persistent.
But there are several important differences:
- WebSocket
  Both client and server can exchange messages | Binary and text data | WebSocket protocol (wss:// or ws://) | Manual code for reconnect
- EventSource
  Only server sends data | Only text | Regular HTTP (https:// or http://) | Supports auto-reconnect
The server can set recommended delay using `retry:` in response (in milliseconds). There’s no way to set it from JavaScript.
The server may specify custom events using `event:` (default type is message). event name, must precede data:
CODE SAMPLE:
- SERVER
  retry: 15000
  event: join
  data: {"KEY1": "VAL1", "KEY2": "VAL2"}                              // join event
  data: {"KEY1": "VAL1", "KEY2": "VAL2", "KEY3": "VAL3"}              // message event
  event: leave
  data: {"KEY1": "VAL1"}                                              // leave event
- JS
  let eventSource = new EventSource("https://another-site.com/events");
  eventSource.onmessage = function(event) {                           // onmessage: similar to WebWorker
    console.log("New message", event.data);
  };
  eventSource.close();

Polling - Long / Short
=========================
Polling is the simplest way of having persistent connection with server,
that doesn’t use any specific protocol like WebSocket or Server Sent Events.
A.Regular Polling or Short Polling
  Regular requests are sent to server: “Hello, I’m here, do you have any information for me?”. For example, once every 10 seconds.
  In response, server takes notice to itself that client is online, and then sends a packet of messages it got till that moment.
  It has some downsides:
  - Messages are passed with a delay up to N seconds (here 10 seconds, between requests).
  - Even if there are no messages, the server is bombed with requests every N seconds,
    even if the user switched somewhere else or is asleep. That’s quite a load performance-wise.
B.Long Polling
  A request is sent to the server. The server doesn’t close the connection until it has a message to send.
  When a message appears – the server responds to the request with it.
  The browser makes a new request immediately.
  Only when a message is delivered, the connection is closed and reestablished.
  If the connection is lost, because of, say, a network error, the browser immediately sends a new request.
  CODE SAMPLE for long polling, where subscribe function makes fetch, then waits for response, handles it and calls itself again:
  async function subscribe() {
    let response = await fetch("/subscribe");
    if (response.status == 502) {
      // Status 502 is a connection timeout error,
      // may happen when the connection was pending for too long, and the remote server or a proxy closed it
      // let's reconnect
      await subscribe();
    } else if (response.status != 200) {
      // An error - let's show it
      showMessage(response.statusText);
      // Reconnect in one second
      await new Promise(resolve => setTimeout(resolve, 1000));
      await subscribe();
    } else {
      // Get and show the message
      let message = await response.text();
      showMessage(message);
      // Call subscribe() again to get the next message
      await subscribe();
    }
  }
  subscribe();

Rate Limiting
===============
It is a technique used in applications to control and manage the rate at which requests are processed by individual services.
The primary objective of implementing rate limiting in microservices is to prevent overloading, maintain service stability and
availability, and mitigate potential security threats, such as Denial of Service (DoS) attacks.
Rate limiting can be applied at different levels, such as API gateway, load balancer, or even on an individual microservice level.
There are different approaches to implementing rate limiting, some of the popular ones being:
- Fixed Window: In this approach, fixed number of requests are allowed per predefined time window (e.g., 1000 requests per minute).
  The disadvantage of this approach is that it can lead to uneven distribution of requests and, in some cases, cause service overload.
- Sliding Window: Rate limit is gradually adjusted based on number of requests received in a recent time window.
  This approach offers better control over the rate of incoming requests.
- Token Bucket: In this method, tokens are generated at a specific rate and added to a bucket.
  Each incoming request consumes a token from the bucket. If there are no tokens available, the request is denied.
  This technique allows for short bursts of requests while maintaining an overall balanced rate

Scaling - Horizontal / Vertical
=================================
- Horizontal scaling involves adding more machines or nodes to a system. It involves scaling out.
  Horizontal scaling is typically used to handle increasing amounts of traffic or workload.
- Vertical scaling involves adding more power (CPU, RAM, storage, etc.) to an existing machine. It involves scaling up.
  Vertical scaling is typically used to handle resource-intensive tasks or applications that require more processing power.

Data Partitioning / Sharding
==============================
- Partitioning is an optimization technique­ in databases where a single­ table is divided into smaller se­gments called partitions.
  It is within a single database instance. It is then used with join queries.
- Sharding involves fragmenting the­ extensive datase­t into smaller, self-contained se­gments known as shards.
  These shards are­ then allocated to separate­ servers or nodes, facilitating paralle­lism in data processing.
  As a result, query re­sponse times are improve­d, high traffic loads can be accommodated.

Service Discovery
===================
In case of a micro-service architecture that is deployed in cloud, there will be a lot of microservice components for different services.
Every microservice can scale differently based on the demands.
Eg: The Order service might have 4 or 5 instances and Billing Service might have 2 or 3 instances running.
With each instance having a dynamic network address because of multiple factors like Autoscaling, Upgrade, Failure, Deployments, etc.
it is extremely difficult to locate these services and communicate with them.
Before server connects to other server it needs to know IP address and port number where this particular application is running in server. 
This problem leads to the solution in the form of a design pattern called Service Discovery.
We need service discovery to find the hosts and IP info and
If multiple hosts are available then the load balancer helps to pick one whoever has less load and make a call in a load-balanced way.
Eg: Whenever Service-A and Service-B start off, they register themselves inside the discovery service's service registry.
    Service Registry is a database containing network locations of each service's instances.
    Service-A wants to connect to Service-B.
    Now the load balancer once get the request, it is gonna do a query with the discovery service that,
    hey, can you tell me what instances are there for Service-B?
    Now the load balancer finds out that there are this many instances available where Service-B has been deployed.
    Load Balancer is going to dispatch to one of the servers by looking into Service Registry.
    It can take all four instances of Service-B and whoever has less load then to balance the load, it can send the request to there.
There are two types of Service Discovery
- Client-Side Service Discovery like Zookeeper, Consul
  Eg: Service-A wants to connect to Service-B then Service-A is asking directly to Discovery Service and
      Discovery Service is providing the URLs or the port number of all instances of Service-B.
      There is no load balancer and client(Service-A) is doing load balancing and calling the Service-B instance by itself.
  Advantage: it saves an extra hop that we would’ve had with a dedicated load balancer.
  Disadvantage: because the Service Consumer must implement the load balancing logic.
- Server-Side Service Discovery like NGINX, AWS ELB
  The default scenario explained above. Here Service-A(Client) doesn’t talk to the Discovery Service directly.
  It calls another server (Load Balancer) which helps to discover Service-B URL info.
  Advantage: load Balancer does the job of load balancing.
  Disadvantage: set up and operate the Load Balancer.

Circuit Breaker Pattern
=========================

MapReduce
===========

Message Queues
================

Load Balancers
================
- When a microservices function requires more computing power,
  only that microservice is scaled by adding more instances of that service through load balancer to share network traffic.
- We can add a Load balancing layer at three places in our system:
  Between Clients and Application servers
  Between Application Servers and database servers
  Between Application Servers and Cache servers
- Round Robin approach distributes incoming requests equally among backend servers.
  If any server is dead, LB will take it out of rotation and will stop sending any traffic to it.
- A problem with Round Robin LB is that we don’t take the server load into consideration.
  If a server is overloaded or slow, LB will not stop sending new requests to that server.
  To handle this, another LB solution can be used that periodically queries backend server about its load & adjusts traffic based on that.

Web Authentication and Security
=================================

Indexing
==========

CAP Theorem
=============

Deployment Types - Rolling / Blue Green / Canary
==================================================

Caching + Edge Caching
========================

A/B Testing
=============
