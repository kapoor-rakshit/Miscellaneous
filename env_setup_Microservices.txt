Microservices Architecture
============================
  Client <---> Load Balancers <---> Application Servers <---> Cache <---> Sharded Databases
- It is an approach to design and implement applications in which large application is built from modular components or services.
- Each module supports a specific task or business goal and uses a well-defined communications interface,
  such as an application programming interface (API), to communicate with other modules and services.
- It is a departure from applications that were coded into a single executable application i.e. monolithic
  Each service can be isolated, rebuilt, tested, redeployed and managed independently.
  eg: if program isn't properly generating reports we can test, restart, patch, redeploy that service only, independent of other services.
- Typical characteristics of microservices design and architecture include:
  - Unique components (individual components for specific function)
  - Decentralized (few dependencies and requires frequent and extensive communication between components)
  - Resilient (fault tolerance)
  - API-based (APIs and API gateways for communication between components and other applications)
  - Data separation (each service accesses its own database or storage volume)
  - Automation (automation and orchestration technologies for component deployment and scaling)
- Challenges of a microservices architecture:
  - Unnecessary complexity
  - Performance issues from Network congestion, latency, network management, control can require multiple load-balancing instances
  - Monitoring health and performance, as well as automation and orchestration
  - Security

Web Socket
============
WebSockets are a technology that allow for real-time communication between a client and server.
They provide a bi-directional communication, without the need for the client to constantly request data from the server.
In traditional web applications, the client sends a request to the server, which then responds with the requested data.
This request-response model can be inefficient for real-time applications, such as
chat applications, online gaming, and financial trading platforms, where data needs to be updated in real-time.
REFER: CODE SAMPLE IN   `angular`   REPOSITORY's   `misc`

Server Sent Events or EventSource
====================================


Polling - Long / Short
=========================
Polling is the simplest way of having persistent connection with server,
that doesn’t use any specific protocol like WebSocket or Server Sent Events.
- Regular Polling or Short Polling
  Regular requests are sent to server: “Hello, I’m here, do you have any information for me?”. For example, once every 10 seconds.
  In response, server takes notice to itself that client is online, and then sends a packet of messages it got till that moment.
  It has some downsides:
  - Messages are passed with a delay up to N seconds (here 10 seconds, between requests).
  - Even if there are no messages, the server is bombed with requests every N seconds,
    even if the user switched somewhere else or is asleep. That’s quite a load performance-wise.
- Long Polling
  A request is sent to the server. The server doesn’t close the connection until it has a message to send.
  When a message appears – the server responds to the request with it.
  The browser makes a new request immediately.
  Only when a message is delivered, the connection is closed and reestablished.
  If the connection is lost, because of, say, a network error, the browser immediately sends a new request.
  CODE SAMPLE for long polling where subscribe function makes fetch, then waits for response, handles it and calls itself again:
  async function subscribe() {
    let response = await fetch("/subscribe");
    if (response.status == 502) {
      // Status 502 is a connection timeout error,
      // may happen when the connection was pending for too long, and the remote server or a proxy closed it
      // let's reconnect
      await subscribe();
    } else if (response.status != 200) {
      // An error - let's show it
      showMessage(response.statusText);
      // Reconnect in one second
      await new Promise(resolve => setTimeout(resolve, 1000));
      await subscribe();
    } else {
      // Get and show the message
      let message = await response.text();
      showMessage(message);
      // Call subscribe() again to get the next message
      await subscribe();
    }
  }
  subscribe();

Rate Limiting
===============
It is a technique used in applications to control and manage the rate at which requests are processed by individual services.
The primary objective of implementing rate limiting in microservices is to prevent overloading, maintain service stability and
availability, and mitigate potential security threats, such as Denial of Service (DoS) attacks.

Scaling - Horizontal / Vertical
=================================

Data Partioning
=================

Service Discovery
===================

Circuit Breaker Pattern
=========================

MapReduce
===========

Message Queues
================

Load Balancers
================
- When a microservices function requires more computing power,
  only that microservice is scaled by adding more instances of that service through load balancer to share network traffic.
- We can add a Load balancing layer at three places in our system:
  Between Clients and Application servers
  Between Application Servers and database servers
  Between Application Servers and Cache servers
- Round Robin approach distributes incoming requests equally among backend servers.
  If any server is dead, LB will take it out of rotation and will stop sending any traffic to it.
- A problem with Round Robin LB is that we don’t take the server load into consideration.
  If a server is overloaded or slow, LB will not stop sending new requests to that server.
  To handle this, another LB solution can be used that periodically queries backend server about its load & adjusts traffic based on that.

Web Authentication and Security
=================================

Indexing
==========

CAP Theorem
=============

Deployment Types - Rolling / Blue Green / Canary
==================================================

Caching + Edge Caching
========================

A/B Testing
=============
