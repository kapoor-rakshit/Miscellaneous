
Developer's     : job is to add new features.
Operations's    : job is to keep site stable and fast.
Tester's        : job is to reduce risk.



DevOps
            It solves customer problems
            It is for cloud
            It is a solution
            It is a tool
            It is a skill to master 

Agile breaks wall between Business and Development team.
DevOps breaks wall between Development and Operations team.



- Dev        : people involved in developing product.
- Ops        : system engineers, administrators, operations staff, release engineers, DBA, network engineers, security engineers.
- Agile      : collaboration of customers, product management, developers, QA
- DevOps     : extending principles of Agile to entire delivered service.
                      - improved deployment frequency.
                      - faster time to market.
                      - lower failure rate.
                      - shortened lead time between fixes.
                      - faster mean time to recovery in event of a new relaease crashing.
- CD          : Continuous Delivery
- CI          : Continuous Integration
- Lead time   : time between identification of requirement and it's fulfillment.
- Mean time to failure : time for which a system or application will last in operation.



DevOps practice involves
-----------------------------------
  PLAN --> CODE --> BUILD --> TEST --> INTEGRATE --> DEPLOY --> OPERATE --> MONITOR
- Continuous Development which needs Version control for all
- Automated and Continuous testing
- Proactive monitoring and metrics
- Configuration management
- Continuous Integration / Delivery / Deployment
- Virtualization / Coud / Containers
- Toolchain approach



DevOps tools
-------------------
Development
     Source Code Management or Continuous Development
           - Git
     Continuous Integration
           - Ant, Maven, Gradle    ( Build Automation tool ie download required dependencies )
           - Jenkins, Travis       ( Platforms for orchestration )
           - Selenium, TestNG      ( WebDriver + Testing )
Operations
      Configuration Management
           - Chef, Puppet, Ansible, Saltstack
      Containers
           - Docker
      Monitoring
           - Nagios


CI workflow
----------------
- Commit changes to source code by developers.
- CI server pulls that code and triggers a build depending on schedule configured in CRON format at jenkins.
- Build application is deployed on testing server for testing.
- After testing application is deployed on production server.
- Concerned teams are constantly notified about build and test results.

Benefits of CI
------------------
- Reduced risk
- Bugs and problems discovered in time
- System is deployable more frequently, enabling more user feedback

Best practices of CI
-------------------------
- Maintain single source repository
- Everyone commits to master branch everyday
- Automate build
- Keep build fast
- Make build self testing
- Fix broken builds immediately
- Everyone can see what's happening
- Automate deployment


Build (Maven)
--------------
- All dependencies are available at Maven Centralized Repo , which can be downloaded and used in our projects.
- Maven Lifecycle
  - Maven Compile         -> Java Classes, Compile -> (achieved by Maven Compiler plugin)
  - Maven Test            -> Test cases            -> (achieved by Maven Surefire plugin)
  - Maven Resource        -> JARs/WARs/EARs        -> (achieved by Maven Resource plugin)
- Maven has a POM (Page Object Model) XML file which contains      <dependencies> <dependency></dependency> </dependencies , <properties></properties>   etc...
  Although we have above plugins already included in project , still it's good practice to write all these plugins in POM.xml under <build> <plugins> <plugin></plugin> </plugins> </build>
- Maven Commands
  - $ mvn clean install        -> executes all three lifecycle procedures
  - $ mvn test                 -> executes only test lifecycle ie all methods with @Test in project
  - $ mvn package -DskipTests  -> skips test lifecycle



Jenkins
---------
- Jenkins is an orchestration and automation engine.
- It is extremely pluggable, with a lot of plug-ins emerged.
- The idea of CI is to merge code from individual developers into a project mulitple times per day and test continuously to avoid downstream problems.
- CD ensures that all merged code is always in a production ready state.
- Jenkins enables developers to automate this process as much as possible upto point of deployment
- STEPS
  ----------------------
  - Download jenkins.war from official website or use commands like "wget"  etc... 
  - On terminal, cd to this dir of downloaded jenkins.war and write command,      $ java -jar jenkins.war --httpPort=9090
    If this is first time , it will provide a password to be entered on home screen of Jenkins
  - open http://localhost:PORT on browser   (PORT = 8080 default or as specified)
  - install required plugins   Manage Jenkins --> Manage Plugins (eg: git plugin) --> search Available for required plugin or in Advanced (upload plugin file)
- Jenkins on TOMCAT
  -----------------------
  - Place jenkins.war in webapps dir of TOMCAT and start TOMCAT
- Jenkins root/HOME dir
  -----------------------
  - By default jenkins root dir ( where we have logs, configs, workspace, builds etc... ) is provided by default by jenkins. 
    Check from Manage Jenkins tab --> Configure Sysytem  once jenkins is running.
  - Copy all files from this old dir ( it might be hidden dir ) to new desired dir.
  - Change or add ENVIRONMENT variables JENKINS_HOME=<Path_to_new_dir>
- Jenkins CLI
  -----------------------
  - Start Jenkins
  - Manage Jenkins tab --> Configure Global Security --> Enable Security
  - Go to browser --> http://localhost:8080/cli/    this page will provide syntax for commands as per our instance
  - Download jenkins-cli.jar
  - cd to this .jar location and type command       java -jar jenkins-cli.jar -s <URL_for_Jenkins> <command>
    This will ask for a passphrase (SSH public key) which can be found under profile tab of user currently logged in --> Configure.
- Jenkins Create Users + Manage Roles
  ------------------------------------
  - Manage Jenkins tab --> Manage Users --> Create User
  - User Profile tab --> Configure
  - Role Strategy Plugin
    - Manage Jenkins --> Manage Plugins --> Available tab (Search for plugin and Download + install) or Advanced tab to upload plugin file
    - Manage Jenkins --> Cofigure Global Security --> Check Enable Security option --> Authorization (check ROLE BASED STRATEGY option)
    - Manage Jenkins --> Manage and Assign Roles --> Manage Roles
      Global roles   : Roles at global level     (eg: admin, employee)
      Project roles  : Roles at project level    (Define pattern (eg: Dev.* or Test.*) for which projects to assign roles) (eg: developer, tester)
    - Manage Jenkins --> Manage and Assign Roles --> Assign Roles
      Global roles   : Select role to assign as given in Manage Roles --> Global role tab     (eg: admin or employee)
      Project roles  : Select role to assign as given in Manage Roles --> Project role tab    (eg: developer or tester)
- Jenkins Configurations
   ---------------------------
  - Manage Jenkins --> Configure System
    Home directory           : home dir of jenkins (this can be changed , check "Jenkins root/HOME dir" step) , advance options will provide Workspace , Builds config
    System Message           : we can use HTML or Plain text ( as set in Manage Jenkins --> Configure Global Security --> Markup Formatter ) visible on dashboard of Jenkins
    Quiet Period             : number of seconds to wait before triggering job
    SCM checkout retry count : Max retry count to connect to SCM remote repo
    Restrict project Naming  : Naming pattern for new project to create
    Jenkins Location         : URL to use on browser for Jenkins UI, can be localhost or IP address
- JOBS (Project/Item) in Jenkins
  -------------------------------
  - New Item --> Select item configurations (maven/freestyle)
    - General         --> Project name, Description
    - SCM             --> Integrate remote repo (git, svn) .git URL, branch
    - Build Triggers  --> when/how to trigger job
       Trigger builds remotely          --> Provide auth token --> use URL provided to trigger job remotely from browser
       Build after other projects built --> Provide projects to watch (stable/unstable/fails)
       Build Periodically               --> CRON syntax followed
       Poll SCM                         --> Poll SCM (.git) integrated , uses CRON syntax to poll for new commit
    - Build           --> execute shell commands on code repo/files   (eg: cd /user/ra20024024/javaProj/; javac myfile.java; java myfile;)
    - Post Build Actions
       Build other projects             --> Provide project to build (stable/unstable/fails)
       Deploy WAR/EAR to container      --> Available via plugin
       Email Notifications
- CATLIGHT
  --------------------------------
  - It is a status notifier for CI tools like Jenkins, Travis etc.. which provides notifications on desktop for JOBS.
     Refer : https://catlight.io/
- Deploy to Container Plugin
  --------------------------------
  - If both Jenkins and TOMCAT are running on same machine , make sure to use different ports for these.
  - Make sure both are up and running plus configurations in case any connectionException occurs.
  - Search for this plugin in Manage Jenkins --> Manage Plugin --> Available --> Search/Download/Install  "Deploy to Container Plugin"
  - In Post Build actions of Item/JOB , choose Deploy WAR/EAR to container. Choose your WAR name (as in <workspace_path>/<project> of Jenkins)
  - For TOMCAT credentials to be used with Jenkins , go to tomcat-users.xml to check and use, <user username="" password="" roles=""></user>
- EMAIL Notifications
  ---------------------------------
  - Refer:    https://www.arclab.com/en/kb/email/list-of-smtp-and-pop3-servers-mailserver-list.html     for SMTP details.
  - Manage Jenkins --> Configure System --> Email Notifications (Advanced). Provide SMTP details from where to send.
  - Inside JOB --> Configure --> Post Build Actions --> Email notification
- Jenkins Delivery PIPELINE
  ---------------------------------
  - To Visualize Chained Jobs.
  - Download/Install Delivery Pipeline Plugin
  - Add new View/Tab on Jenkins Dashboard and select Delivery Pipeline View.
  - Configure this view , component and add initial job , Instances to show , Enable rebuild , Enable start , Enable Build Time   etc...
- Jenkins Build PIPELINE
  ---------------------------------
  - To Visualize Chained Jobs.
  - Download/Install Build Pipeline Plugin
  - Add new View/Tab on Jenkins Dashboard and select Build Pipeline View.
  - Configure this view , initial job , Instances to show , Console output  etc...
- Jenkins Blue Ocean UI
  ---------------------------------
  - Works only on Jenkins version 2.7 or above.
  - A new UI for Jenkins that provides interactive view for Jenkins pipeline and jobs.
  - Download/Install Blue Ocean plugin.
  - A button will appear on dashboard to switch UI to Blue Ocean UI and similarly in Blue Ocean to switch to classic view.
- Poll Mail Box Trigger
  ---------------------------------
  - Trigger Build on receiving a mail
  - Download/Install Poll Mail Box Trigger plugin
  - Configure plugin under Build Triggers
- Build Monitor View plugin
  ---------------------------------
  - Shows highly visible status of selected jobs.
  - Download/Install Build monitor view plugin.
  - Add new View/Tab on Jenkins Dashboard and select Build Monitor View
  - Configure plugin.



Continuous Deployment (CD)
---------------------------
- It is a business specific need and might not always required to be performed.
- Blue Green deployment method
  - A technique to reduce downtime and risk by running two identical production environments called Blue and Green
  - At anytime only one of environments is live , serving all production traffic.
- Rollback also is an important part of automated deployment.

CD enablers
----------------
- Comprehensive configuration management
- Containers
- Continuous Testing

Benefits of CD
--------------------
- Low risk releases
- Faster time to market
- Higher quality
- Lower costs
- Better products



CM ( Configuration Management )
--------------------------------------------
- To establish and maintain integrity of products of software project.
- It refers to discipline for evaluating, coordinating, approving or disapproving and implementing changes in artifacts that are used to construct and maintain software.
- An artifact is a piece of hardware or software or documentation.
- This makes use of IAC (Infrastructure as Code).
- It controls changes , if a software upgrade or configuration caused glitch , it can be rolled back easily using CM tools.
- Without CM, a common issue of code working on Dev environment and not working on Production environment may occur.
- It allows to scale the system by having one control machine and others can be remote machines to scale to.

Ansible
----------
- Ansible uses Playbook which uses YAML (.yml) files.
- It does not require any agents whereas Chef or Puppet does require , It is built on python , It uses SSH.
- It uses Push based architecture whereas Chef or Puppet uses Pull based architecture.
- YAML (.yml) files
  - It starts with   ---  and ends with   ...
  - Lists are also written in YAML each element in new line
  - specific tags are specified in YAML like name, hosts, vars, tasks
- STEPS
  - genearte SSH key using `ssh-keygen`
  - modify HOST inventory file located at /etc/ansible/hosts to specify group or individual HOST
  - copy SSH key to remote machine using `ssh-copy-id -i HOSTNAME`
  - check if remote machine or group is up and running as mentioned in HOST inventory file using `ansible -m ping test-servers`
  - create a YAML (.yml) file
  - run playbook with this YAML file using `ansible-playbook myfile.yml`

Chef
----------
- Recipe is a configuration script used in Chef.
- Chef is written in Ruby and recipes are written in Ruby too.
- Collection of scripts (recipes) is called CookBook.
- Tool with which scripts created, it is called Knife.
- It makes use of Pull based approach where nodes poll server and pull configurations.
- Chef Architecture
  Three entities Workstation, Chef Server, Nodes.
  Workstation is like development machine or local chef repository from where configurations are sent to Chef Server.
  Chef server is like remote repository or a mirror for Nodes which tells them (nodes) whether they are inline with configurations desired.
  Nodes (chef clients) are target systems which pulls configuration from Chef Server that needs to be applied to them.
- To install chef , download Chef Development Kit (CDK).

Puppet
----------
- It makes use of Pull based approach.
- Puppet Architecture
  It has a Master-Slave architecture.
  Nodes (slaves or agent) send data about itself to Puppet Master , ie it polls master.
  This data is then compiled by Puppet master and configuration data is sent back to node.
  Node then sends back a confirmation to Master that configuration is complete and it is visible on dashboard.
- Resources are files, packages that define some aspect of system.
  eg: <resource_type> { '<resource_name>' : <attribute> => '<value>' }
- Classes are groups of resources. A resource defines a single file or package but a class describes everything needed to configure entire system.
  eg: class <example_class> { ... }
- Manifests are Puppet programs which have extensions as .pp
- Modules are collection of manifests and data.
  To add module to puppet , place it in   `/etc/puppet/modules`   directory




CONTAINERIZATION
-------------------------------------
dockerfile
------------
- A file (without any extension) with instructions to build image.
- It uses   #   for comments.
- FROM <base_image_name>:<tag_name>                // this can be "scratch" as per dockerhub documentation if we donot want any base image
  ENV <key>=<val>                                  // a key-val pair that can be accessed using <key> throughout dockerfile
  LABEL maintainer=<email_id>                      // best practice to specify maintainer using LABEL as key-val pair
  WORKDIR /usr/src/app                             // sets working directory for instructions (COPY, RUN,...) that follow it in dockerfile
  ADD <src> <dest>                                 // works similar to COPY but support, use of URL instead of a local file <src>, extract tar file from source directly into destination.
  COPY package*.json ./                            // copies files or directories from <src> (eg: package*.json) to <dest> (eg: ./)
  RUN npm install                                  // this will execute while building image from dockerfile
  COPY . ./                                        // bundle your app's entire source code inside the Docker image , NOTE: use of "."
  EXPOSE 8080                                      // informs Docker that container listens on the specified network ports at runtime , however during runtime we can map machine port to this exposed port using -p flag
  CMD ["node", "app.js"]                           // this will execute while running container from image
  
.dockerignore
----------------
- A .dockerignore is similar to .gitignore , which will ignore files to be added/copied to image

Images + Containers
---------------------
- It packages application and it's dependencies together in form of a container (run time instance of docker image)
- Containers are used by a container engine (Docker Engine) which is there on host OS.
- RAM consumed by containers is very less than that required by VM (virtual machine).
- Easily run applications containers , no need to reboot entire VM.
- Docker Images are light weight templates and shared via Docker Hub (online repository) or Import/Export.
- An unused image means that it has not been assigned or used in a container i.e. command "docker ps -a" will not show unused image.
- A dangling image means that you've created the new build of the image, but it wasn't given a new name. 
  So the old images you have becomes the "dangling image" and command "docker images -a" will show "<none>" as their name.

Virtualization vs Containerization
------------------------------------
- In virtualization we have a software called Hypervisor which will create multiple GUEST OS over HOST OS.
  Each of these GUEST OS are a VM (app plus guest OS) and they donot use HOST OS which is an overhead over HOST platform.
  Each of VM have a defined memory which does not change as per application needs resulting in wastage and redundant memory.
- In containerization we have Container Engine (Docker Engine) which uses containers.
  Each of continers (app plus libs) uses memory as per application needs from OS resulting in efficient use of HOST platform.
  It can run on any machine that has Docker installed which prevents need to configure and build app multiple times on different platforms.

How are containers created ?
-----------------------------
- Docker file (Dockerfile) which contains commands are used to create Docker image and that image will contain all project codes.
- Docker image can be used to spin "N" number of containers each with modifications to underlying image.
- Docker image is uploaded on Docker Hub and is pulled on staging env or testing env where docker containers are spun and code can run.

Docker architecture + commands
--------------------------------
- Docker is written in Go programming language.
- Docker has a client (Command Line) - server (Docker Daemon) architecture.
  server has containers which respond to REST API request or Command from terminal coming from either same or different machine as of server
  client and server together form Docker Engine.
- Install
  $ sudo apt-get update
  $ sudo apt-get install docker
- Add user to docker group to avoid writing "sudo" before every command
  $ sudo usermod -a -G docker <username>
  "Relogin to system for this to take effect"
- Start
  $ sudo service docker start                        // start docker service                        
- Stop
  $ sudo service docker stop                         // stop docker service
- Misc. commands
  $ docker version                                   // information of docker client-server
  $ docker info                                      // information of containers + images + server
  $ docker <command_name> --help                     // manual to <commandname>
  $ docker login                                     // login to dockerhub
  $ docker stats                                     // realtime stats of all containers
  $ docker system df                                 // disk usage by docker
  $ docker system prune                              // removes containers/images
- Images
  $ docker images -a                                 // list of images
  $ docker pull <image_name>                         // pull image with default tag "latest" from dockerhub
  $ docker pull <image_name>:<tag_name>              // pull image with tag "<tag_name>" from dockerhub     ,  NOTE: use of ":"
  $ docker build -t <image_name>:<tag_name> <PATH>   // build image from dockerfile located at PATH dir, -t: name and optional tag
  $ docker build -t <image_name>:<tag_name> .        // build image from dockerfile located at current dir  ,  NOTE: use of "."
  $ docker rmi <image_name>/<imageID>                // remove one or more images
- Containers
  $ docker ps                                        // lists currently running containers
  $ docker ps -a                                     // list all containers
  $ docker run <image_name>                          // run container from image <image_name>
  $ docker run <image_name>:<tag_name>               // run container from image <image_name> with tag <tag_name>
  $ docker run --name <customized_name> <image_name> // run container with a <customized_name> from image <image_name>
  $ docker start <customized_name>/<containerID>     // start container
  $ docker stop <customized_name>/<containerID>      // stop container
  $ docker pause <customized_name>/<containerID>     // pause running container
  $ docker unpause <customized_name>/<containerID>   // resume paused container
  $ docker stats <customized_name>/<containerID>     // stats for particular container
  $ docker attach <customized_name>/<containerID>    // work over running container in new terminal
  $ docker rm <customized_name>/<containerID>        // remove container
  
Docker Volume
---------------
- In absence of volumes data generated and used by container is stored within container, incase a container is deleted, entire data is also lost.
- To have a persistent storage for data generated and used by containers we create VOLUMES, that are independent of container.
  It allows for shared volume (data/storage) among different containers.
  It allows to attach a volume to a container.
- Create volume
  $ docker volume create <VOL_NAME>
- List volumes
  $ docker volume ls
- Info about a volume
  $ docker volume inspect <VOL_NAME>
- Remove a volume
  $ docker volume rm <VOL_NAME>
  $ docker volume prune                             // remove all unused volumes (not used by any container)
- Example
  $ docker run --name <customized_name> -p 9090:8080 -p 60000:50000 -v <VOL_NAME>:/var/jenkins_home jenkins      // using VOLUMES
  $ docker run --name <customized_name> -p 9090:8080 -p 50000:50000 -v <PATH_to_DIR>:/var/jenkins_home jenkins   // using BIND MOUNTS
  
Jenkins + Docker
------------------
- Pull jenkins image from docker hub
  $ docker pull jenkins
- Run jenkins image and expose port 9090 , 50000 of local to 8080 of docker server and 50000 of Jenkins API plus provide persistent storage to JENKINS_HOME.
  This will retain all data (plugins + credentials) even if Jenkins container is stopped.
  $ docker run --name <customized_name> -p 9090:8080 -p 50000:50000 -v <PATH_to_DIR>:/var/jenkins_home jenkins

Docker Swarm
--------------
- A swarm is a group of machines (either physical or virtual) that are running docker and joined into a cluster.
- Docker swarm is a tool for container orchestration just like Kubernetes used for managing and controlling multiple docker containers a a single service.
  It ensures health check of each container on each system.
  Scaling containers up/down as per load.
  Adding updates to containers.
- Docker Machine
  docker-machine enables us to create Docker hosts either on local or remote and make one host as manager node and other host as worker node for Docker Swarm.
  We can add one or more nodes as manager to act as backup in case any manager node opts-out. But there is only one leader at any time.
  $ docker-machine create --driver hyperv <NODE_NAME>                  // WINDOWS
  $ docker-machine create --driver virtualbox <NODE_NAME>              // MAC
  $ docker-machine env <NODE_NAME>                                     // gives details about machine created
  $ docker-machine ls                                                  // lists all docker machines with their details
  $ docker-machine ssh <NODE_NAME>                                     // connect to <NODE_NAME>
  $ docker-machine stop <NODE_NAME>                                    // stop/drain machine 
  $ docker-machine rm <NODE_NAME>                                      // remove machine
  
  $ docker swarm init --advertise-addr <MANAGER_NODE_IP>               // NOTE : Write this command at MANAGER NODE cmd to let it know that it is a manager.
  $ docker swarm join-token worker                                     // NOTE : Write this command at MANAGER NODE cmd to generate token command for a WORKER NODE cmd to join swarm as worker. Copy and run this in node cmd to act it as worker.
  $ docker swarm join-token manager                                    // NOTE : Write this command at MANAGER NODE cmd to generate token command for a MANAGER NODE cmd to join swarm as manager. Copy and run this in node cmd to act it as manager.
  $ docker swarm leave                                                 // to leave swarm (can be manager or worker node)
  
  $ docker node ls                                                     // NOTE : Write this command at MANAGER NODE cmd to have info about swarm
  $ docker node update --availability drain <NODE_NAME>                // NOTE : Write this command at MANAGER NODE cmd to shutdown node of swarm.
  
  $ docker service create --replicas <NO_OF_NODES> -p 80:80 --name <customized_service_name> <IMAGE_NAME>    // NOTE : Write this command at MANAGER NODE cmd to replicate service on all nodes including manager.
  $ docker service ps <customized_service_name>                        // to have info about <customized_service_name>
  $ docker service scale <customized_service_name>=<NO_OF_REPLICAS>    // NOTE : Write this command at MANAGER NODE cmd to spin up/down a container on any random node of swarm
  $ docker service update --image <IMAGE_NAME>:<TAG_NAME> <customized_service_name>     // updates service
  $ docker service rm <customized_service_name>                                         // remove service
  
Docker Compose
------------------
- It is a tool for defining and running multi-container docker applications where multiple containers work together to provide required functionality.
  It uses yaml files for configuration and named as     docker-compose.yml
  We can start/stop all services with a single command  $ docker compose up/down
  We can scale selected services as and when required.
- Docker Swarm runs multi-container applications, just like Compose does.
  The key difference is that Swarm schedules and manages your containers across multiple machines, while Compose schedules and manages containers on a single host only.
- Install
  $ python -m pip install -U docker-compose
- Docker-Compose.yml
  Create a file named as "docker-compose.yml"
  Write a YAML file for configuration which has services: web: image: database: image:
  REFERENCE : docker-compose documentation for image used available at docker hub
- Check validity
  $ docker-compose config
- Run docker-compose.yml
  $ docker-compose up -d                                // run docker-compose in detached mode to have terminal available for other work
  $ docker-compose up -d --scale <service_name>=4       // scale a service (eg: database to have it's 4 instances or containers running)
- Check running containers
  $ docker-compose ps
- Stop docker-compose
  $ docker-compose down



